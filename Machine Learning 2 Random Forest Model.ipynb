{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 2 Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code focuses on sampling data that can be used in the model building. As the data sets contain many rows, we use sampling to reduce size. We believe this is what makes it hard for the model to capture any patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of work and changes has been done in this file, meaning the code might not be streamlined or #cleancode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LogNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = r'I:\\CME538 Project\\processed data ontario only\\250m\\Approximations\\compiled_climated_files'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run a loop through our files to sample our training and test data. Changes in the code here will determine what years and months the data will consist of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Set the range of numbers\n",
    "start_range = 1\n",
    "end_range = 15123785\n",
    "num_elements = int(0.01 * (end_range - start_range + 1))\n",
    "fraction = 0.1\n",
    "increase = 2.0\n",
    "# Randomly select numbers for sampling\n",
    "\n",
    "\n",
    "# Initialize DataFrames for storing shifted data\n",
    "test = pd.DataFrame()\n",
    "\n",
    "# Iterate through years\n",
    "for year in range(2019, 2022):\n",
    "    test_temp = pd.DataFrame()\n",
    "    # Iterate through months\n",
    "    #for month in [[5,6],[6,7],[7,8],[8,9]]:\n",
    "    for month in [[7,8]]:\n",
    "        random_numbers = random.sample(range(start_range, end_range + 1), num_elements)\n",
    "    # Check if the month is within the summer months\n",
    "        print(\"Processing year:\", year, \"month:\", month[0], \"and\")\n",
    "        print(\"Processing year:\", year, \"month:\", month[1])\n",
    "        # Iterate through files in the directory\n",
    "        for file_name in os.listdir(directory_path):\n",
    "            if (file_name.endswith('.parquet')) & (str(month[0]) in file_name) & (str(year) in file_name):\n",
    "                file_name1 = file_name\n",
    "                parts1 = file_name.split('_')\n",
    "                file_year1 = int(parts[0])\n",
    "                file_month1 = int(parts[1])\n",
    "            if (file_name.endswith('.parquet')) & (str(month[1]) in file_name)& (str(year) in file_name):\n",
    "                file_name2 = file_name\n",
    "                parts2 = file_name.split('_')\n",
    "                file_year2 = int(parts[0])\n",
    "                file_month2 = int(parts[1])\n",
    "            \n",
    "        file_path1 = os.path.join(directory_path, file_name1)\n",
    "        file_path2 = os.path.join(directory_path, file_name2)\n",
    "        data_1 = pd.read_parquet(file_path1, engine='pyarrow')\n",
    "        data_2 = pd.read_parquet(file_path2, engine='pyarrow')\n",
    "        # Extract age and vol columns\n",
    "        age_vol_temp = data_1[['age', 'vol']]\n",
    "        rest = data_2[['burned', 'Tm', 'Tx', 'Tn', 'P']]\n",
    "\n",
    "        data_merged = age_vol_temp.join(rest)\n",
    "        print(len(data_merged))\n",
    "        data_merged = data_merged[data_merged.age >= 0]\n",
    "        data_merged = data_merged[data_merged.age >= 0]\n",
    "        data_merged = data_merged[~data_merged.isnull().any(axis=1)]\n",
    "        print(len(data_merged))\n",
    "        test_temp = pd.concat([test_temp,data_merged.sample(frac=fraction)])\n",
    "        print(len(test_temp))\n",
    "        del data_merged\n",
    "    test = pd.concat([test,test_temp])\n",
    "    print(len(test))\n",
    "# Print the final concatenated DataFrame\n",
    "test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the range of numbers\n",
    "start_range = 1\n",
    "end_range = 15123785\n",
    "num_elements = int(0.01 * (end_range - start_range + 1))\n",
    "fraction = 1\n",
    "increase = 1\n",
    "# Randomly select numbers for sampling\n",
    "\n",
    "\n",
    "# Initialize DataFrames for storing shifted data\n",
    "train = pd.DataFrame()\n",
    "\n",
    "# Iterate through years\n",
    "for year in range(2001, 2020):\n",
    "    train_temp = pd.DataFrame()\n",
    "    # Iterate through months\n",
    "    #for month in [[5,6],[6,7],[7,8],[8,9]]:\n",
    "    print(year)\n",
    "    for month in [[7,8]]:\n",
    "        random_numbers = random.sample(range(start_range, end_range + 1), num_elements)\n",
    "    # Check if the month is within the summer months\n",
    "        print(\"Processing year:\", year, \"month:\", month[0], \"and\")\n",
    "        print(\"Processing year:\", year, \"month:\", month[1])\n",
    "        # Iterate through files in the directory\n",
    "        for file_name in os.listdir(directory_path):\n",
    "            if (file_name.endswith('.parquet')) & (str(month[0]) in file_name) & (str(year) in file_name):\n",
    "                file_name1 = file_name\n",
    "                parts1 = file_name.split('_')\n",
    "                file_year1 = int(parts[0])\n",
    "                file_month1 = int(parts[1])\n",
    "            if (file_name.endswith('.parquet')) & (str(month[1]) in file_name)& (str(year) in file_name):\n",
    "                file_name2 = file_name\n",
    "                parts2 = file_name.split('_')\n",
    "                file_year2 = int(parts[0])\n",
    "                file_month2 = int(parts[1])\n",
    "            \n",
    "        file_path1 = os.path.join(directory_path, file_name1)\n",
    "        file_path2 = os.path.join(directory_path, file_name2)\n",
    "        data_1 = pd.read_parquet(file_path1, engine='pyarrow')\n",
    "        data_2 = pd.read_parquet(file_path2, engine='pyarrow')\n",
    "        # Extract age and vol columns\n",
    "        age_vol_temp = data_1[['age', 'vol']]\n",
    "        rest = data_2[['burned', 'Tm', 'Tx', 'Tn', 'P']]\n",
    "\n",
    "        data_merged = age_vol_temp.join(rest)\n",
    "        print(len(data_merged))\n",
    "        data_merged = data_merged[data_merged.age >= 0]\n",
    "        data_merged = data_merged[data_merged.age >= 0]\n",
    "        data_merged = data_merged[~data_merged.isnull().any(axis=1)]\n",
    "        print(len(data_merged))\n",
    "\n",
    "\n",
    "\n",
    "        data_merged_ones = data_merged[data_merged.burned == 1]#.sample(frac=fraction)\n",
    "        print(len(data_merged_ones))\n",
    "        data_merged_zeros = data_merged[data_merged.burned == 0]\n",
    "        fraction_zero = (len(data_merged_ones)*fraction)/len(data_merged_zeros)*increase\n",
    "        #print(fraction_zero)\n",
    "        data_merged_zeros = data_merged_zeros.sample(frac=fraction_zero)\n",
    "        print(len(data_merged_zeros))\n",
    "        #\n",
    "        data_merged = pd.concat([data_merged_ones,data_merged_zeros])\n",
    "        #print(len(data_merged))\n",
    "        data_merged = data_merged.sample(frac = 1)\n",
    "        #print(len(data_merged))\n",
    "        # Merge the two DataFrames on the common rows\n",
    "        train_temp = pd.concat([train_temp,data_merged])\n",
    "        del data_merged\n",
    "        print(len(train_temp))\n",
    "    train = pd.concat([train,train_temp])\n",
    "    print(len(train))\n",
    "# Print the final concatenated DataFrame\n",
    "train.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns that happen to have too many NaN values aren't suitable for a simple model like the one we are building. More analysis on these columns during the processing of the climate data could make it useful. They are, however, omitted from this model. (BS: Hours of bright sunshine, B%S: Hours of bright sunshine compared to normal values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some cleaning of the X values is needed in the feature engineering. NaN values are filled using the imputer tool from the SciKit-library. The age-generating function also happened to create some negative values. We'll just remove the rows affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(data):\n",
    "\n",
    "    # Change features here.\n",
    "    feature_selected = ['age', 'vol', 'Tm', 'Tx', 'Tn', 'P'] \n",
    "    \n",
    "\n",
    "    X = data[feature_selected]\n",
    "    y = data['burned']\n",
    "\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X = imputer.fit_transform(X)\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = feature_engineering(train)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imbalance is taken in account for by using class_weights, making the model more sensitive to patterns in the status 1 class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = y_train.value_counts()\n",
    "class_weights = {0: 1, 1: class_counts[0] / class_counts[1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select some hyperparameters for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLR = RandomForestClassifier(max_depth = 30, class_weight = class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-14 {color: black;}#sk-container-id-14 pre{padding: 0;}#sk-container-id-14 div.sk-toggleable {background-color: white;}#sk-container-id-14 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-14 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-14 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-14 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-14 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-14 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-14 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-14 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-14 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-14 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-14 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-14 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-14 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-14 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-14 div.sk-item {position: relative;z-index: 1;}#sk-container-id-14 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-14 div.sk-item::before, #sk-container-id-14 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-14 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-14 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-14 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-14 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-14 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-14 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-14 div.sk-label-container {text-align: center;}#sk-container-id-14 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-14 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-14\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(class_weight={0: 1, 1: 1.002863571282471}, max_depth=30)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" checked><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(class_weight={0: 1, 1: 1.002863571282471}, max_depth=30)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(class_weight={0: 1, 1: 1.002863571282471}, max_depth=30)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelLR.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a prediction using the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = modelLR.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And print the F1-score from the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On to the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This  model is evaluated using the test set that was separated in the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = feature_engineering(test)\n",
    "y_test_pred = modelLR.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", vmin=0.4, vmax=.4, cbar=False, linewidths=0.5, linecolor='black')\n",
    "plt.title(\"Burned, Final Model\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.savefig('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_test_pred,  digits=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a feature importance plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(model, 'feature_importances_'):\n",
    "    feature_importance = model.feature_importances_\n",
    "    features = ['age', 'vol', 'Tm', 'Tx', 'Tn', 'P']\n",
    "    plt.bar(features, feature_importance)\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.show()\n",
    "    plt.savefig('Feature Importance')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
